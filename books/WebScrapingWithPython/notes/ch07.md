---
title: "Ch07 Web Crawling Models"
category:
 - Programming
tag:
 - Python
 - Web Scraping
---

## 1. Dealing with Different Website Layouts

```python
# 07.1.py
# Dealing with Different Website Layouts

from urllib.request import urlopen
from bs4 import BeautifulSoup


class Content:
    """
    Common base class for all articles
    """

    def __init__(self, url, title, body):
        self.url = url
        self.title = title
        self.body = body

    def print(self):
        print(f"Title: {self.title}")
        print(f"URL: {self.url}")
        print(f"Body: {self.body}")


class Website:
    """
    Contains information about website structure
    """

    def __init__(self, name, url, title_tag, body_tag):
        self.url = url
        self.name = name
        self.title_tag = title_tag
        self.body_tag = body_tag


class Crawler:
    @classmethod
    def get_page(cls, url):
        try:
            html = urlopen(url)
        except Exception:
            return None
        return BeautifulSoup(html, "html.parser")

    @classmethod
    def safe_get(cls, bs, selector):
        """
        Gets content from a bs object and a selector
        """
        selected_elements = bs.select(selector)
        if selected_elements is not None and len(selected_elements) > 0:
            return "\n".join([elem.get_text() for elem in selected_elements])
        return ""

    @classmethod
    def get_content(cls, website, path):
        """
        Extract content from a given page URL
        """
        url = website.url + path
        bs = Crawler.get_page(url)
        if bs is not None:
            title = Crawler.safe_get(bs, website.title_tag)
            body = Crawler.safe_get(bs, website.body_tag)
            return Content(url, title, body)
        return Content(url, "", "")


siteData = [
    ["O'Reilly", "https://www.oreilly.com", "h1", "div.title-description"],
    ["Reuters", "https://www.reuters.com", "h1", "div.ArticleBodyWrapper"],
    ["Brookings", "https://www.brookings.edu", "h1", "div.post-body"],
    ["CNN", "https://www.cnn.com", "h1", "div.article__content"],
]

websites = []
for name, url, title, body in siteData:
    websites.append(Website(name, url, title, body))

Crawler.get_content(
    websites[0], "/library/view/web-scraping-with/9781491910283"
).print()
Crawler.get_content(websites[1], "/article/us-usa-epa-pruitt-idUSKBN19W2D0").print()
Crawler.get_content(
    websites[2],
    "/blog/techtank/2016/03/01/idea-to-retire-old-methods-of-policy-education/",
).print()
Crawler.get_content(
    websites[3], "/2023/04/03/investing/dogecoin-elon-musk-twitter/index.html"
).print()

```

## 2. Structuring Crawlers

### 2.1 Crawling Sites Through Search

```python
# 07.2.py

from urllib.request import urlopen

from bs4 import BeautifulSoup


class Content:
    """
    Common base class for articles
    """

    def __init__(self, topic, url, title, body):
        self.topic = topic
        self.url = url
        self.title = title
        self.body = body

    def print(self):
        """
        Flexible printing function controls output
        """
        print("New article found topic: {}".format(self.topic))
        print("URL: {}".format(self.url))
        print("Title: {}".format(self.title))
        print("Body:\n {}".format(self.body))


class Website:
    """
    Contains information about website structure
    """

    def __init__(
        self,
        name,
        url,
        search_url,
        result_list,
        result_url,
        abs_url,
        title_tag,
        body_tag,
    ):
        self.name = name
        self.url = url
        self.search_url = search_url
        self.result_list = result_list
        self.result_url = result_url
        self.abs_url = abs_url
        self.title_tag = title_tag
        self.body_tag = body_tag


class Crawler:
    def __init__(self, website):
        self.site = website
        self.found = {}

    @classmethod
    def get_page(cls, url):
        try:
            html = urlopen(url)
        except Exception:
            return None
        return BeautifulSoup(html, "html.parser")

    @classmethod
    def safe_get(cls, bs, selector):
        selected_elements = bs.select(selector)
        if selected_elements is not None and len(selected_elements) > 0:
            return "\n".join([elem.get_text() for elem in selected_elements])
        return ""

    def get_content(self, topic, url):
        """
        Extract content from a given page URL
        """
        bs = Crawler.get_page(url)
        if bs is not None:
            title = Crawler.safe_get(bs, self.site.title_tag)
            body = Crawler.safe_get(bs, self.site.body_tag)
            return Content(topic, url, title, body)
        return Content(topic, url, "", "")

    def search(self, topic):
        """
        Searches a given website for a given topic and records all pages found
        """
        bs = Crawler.get_page(self.site.search_url + topic)
        results = bs.select(self.site.result_list)

        for result in results:
            url = result.select(self.site.result_url)[0].attrs["href"]
            # check to see whether it's a relative or an absolute URL
            url = url if self.site.abs_url else self.site.url + url
            if url not in self.found:
                self.found[url] = self.get_content(topic, url)
            self.found[url].print()


siteData = [
    [
        "Reuters",
        "https://reuters.com",
        "https://www.reuters.com/site-search/?query=",
        "div[class*=search-results__list]",
        "header.heder a",
        False,
        "h1",
        "div[class*=article-body__wrapper]",
    ],
    [
        "Brookings",
        "https://www.brookings.edu",
        "https://www.brookings.edu/search/?s=",
        "div.article-info",
        "h4.title a",
        True,
        "h1",
        "div.core-block",
    ],
]

sites = []

for name, url, search, rListing, rUrl, absUrl, tt, bt in siteData:
    sites.append(Website(name, url, search, rListing, rUrl, absUrl, tt, bt))

crawlers = [Crawler(site) for site in sites]
topics = ["python", "data%20science"]

for topic in topics:
    for crawler in crawlers:
        crawler.search(topic)

```

### 2.2 Crawling Sites Through Links

```python
# 07.3.py
# Crawling Sites Through Links

from urllib.request import urlopen
from bs4 import BeautifulSoup

import re


class Website:
    def __init__(self, name, url, pattern, abs_url, title_tag, body_tag):
        self.name = name
        self.url = url
        self.pattern = pattern
        self.abs_url = abs_url
        self.title_tag = title_tag
        self.body_tag = body_tag


class Content:
    def __init__(self, url, title, body):
        self.url = url
        self.title = title
        self.body = body

    def print(self):
        print(f"URL: {self.url}")
        print(f"Title: {self.title}")
        print(f"Body: \n{self.body}")


class Crawler:
    def __init__(self, site):
        self.site = site
        self.visited = {}

    @classmethod
    def get_page(cls, url):
        try:
            html = urlopen(url)
        except Exception as e:
            print(f"failed to open {url}: {e}")
            return None
        return BeautifulSoup(html, "html.parser")

    @classmethod
    def safe_get(cls, bs, selector):
        selected_elems = bs.select(selector)
        if selected_elems is not None and len(selected_elems) > 0:
            return "\n".join([elem.get_text() for elem in selected_elems])
        return ""

    def get_content(self, url):
        bs = Crawler.get_page(url)
        if bs is not None:
            title = Crawler.safe_get(bs, self.site.title_tag)
            body = Crawler.safe_get(bs, self.site.body_tag)
            return Content(url, title, body)
        return Content(url, "", "")

    def crawl(self):
        bs = Crawler.get_page(self.site.url)
        targets = bs.find_all("a", href=re.compile(self.site.pattern))
        for target in targets:
            url = target.attrs["href"]
            url = url if self.site.abs_url else f"{self.site.url}{url}"
            if url not in self.visited:
                self.visited[url] = self.get_content(url)
                self.visited[url].print()


brookings = Website(
    "Brookings",
    "https://brookings.edu",
    "\/(research|blog)\/",
    True,
    "h1",
    "div.post-body",
)

crawler = Crawler(brookings)
crawler.crawl()

```

### 2.3 Crawling Multiple Page Types

- URL
- The presence or lack of certain fields on a site
- The presence of certain tags on the page to identify the page

