---
title: "Ch06 Writing Web Crawlers"
category:
 - Programming
tag:
 - Python
 - Web Scraping
---

## 1. Travering a Single Domain

```python
# 06.1.py
# article links crawler

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re
import random
import time

random.seed(time.time())

wiki_url = "http://en.wikipedia.org{}"


def get_links(article_url):
    html = urlopen(wiki_url.format(article_url))

    bs = BeautifulSoup(html, "html.parser")
    article_reg = r"^(/wiki/)((?!:).)*$"
    return bs.find("div", {"id": "bodyContent"}).find_all(
        "a", href=re.compile(article_reg)
    )


links = get_links("/wiki/Kevin_Bacon")
while len(links) > 0:
    new_article = links[random.randint(0, len(links) - 1)].attrs["href"]
    print(new_article)
    links = get_links(new_article)

```

- `wiki_url.format(article_url)`: 

  `S.format(*args, **kwargs) -> str`

  Return a formatted version of S, using substitutions from args and kwargs.
  The substitutions are identified by braces ('{' and '}').

- `random.randint(0, len(links) - 1)`
  `randint(a,b)`: return a random int in [a, b]

## 2. Crawling the Entire Site

```python
# 06.2.py
# Crawling the Entire Site

from urllib.request import urlopen

from bs4 import BeautifulSoup

import re

wiki_url_prefix = "http://en.wikipedia.org{}"


pages = set()


def get_links(page_url):
    html = urlopen(wiki_url_prefix.format(page_url))
    bs = BeautifulSoup(html, "html.parser")

    link_reg = r"^(/wiki/)"
    for link in bs.find_all("a", href=re.compile(link_reg)):
        if "href" in link.attrs:
            if link.attrs["href"] not in pages:
                # add new pages
                new_page = link.attrs["href"]
                print(new_page)
                pages.add(new_page)

                get_links(new_page)


# start from the home page
get_links("")

```

### 2.1 Collecting Data Across an Entire Site

```python
# 06.3.py
# Collecting data across an entire site

from urllib.request import urlopen
from bs4 import BeautifulSoup

import re

wiki_base_url = "http://en.wikipedia.org{}"

pages = set()


def get_links(page_url):
    html = urlopen(wiki_base_url.format(page_url))
    bs = BeautifulSoup(html, "html.parser")
    try:
        print(bs.h1.get_text())
        print(bs.find(id="mw-content-text").find_all("p")[0])
        print(bs.find(id="ca-edit").find("span").find("a").attrs["href"])
    except AttributeError:
        print("This page is missing something, continuing")

    link_reg = r"^(/wiki/)"
    for link in bs.find_all("a", href=re.compile(link_reg)):
        if "href" in link.attrs:
            if link.attrs["href"] not in pages:
                new_page = link.attrs["href"]
                print("-" * 20)
                print(new_page)
                pages.add(new_page)
                get_links(new_page)


get_links("")

```

## 3. Crawling Across the Internet

```python
# 06.4.py
# Crawling Across the Internet

from urllib.request import urlopen
from urllib.parse import urlparse

from bs4 import BeautifulSoup

import random
import time

random.seed(time.time())


# Retrieve a list of all Internal links found on a page
def get_internal_links(bs, url):
    netloc = urlparse(url).netloc
    scheme = urlparse(url).scheme

    internal_links = set()
    for link in bs.find_all("a"):
        if "href" not in link.attrs:
            continue

        u = link.attrs["href"]
        parsed = urlparse(u)
        if parsed.netloc == "":
            l = f"{scheme}://{netloc}/{u.strip('/')}"
            internal_links.add(l)
        elif parsed.netloc == netloc:
            internal_links.add(u)

    return list(internal_links)


# Retrieves a list of all external links found on a page
def get_external_links(bs, url):
    netloc = urlparse(url).netloc

    extern_links = set()
    for link in bs.find_all("a"):
        if "href" not in link.attrs:
            continue

        u = link.attrs["href"]
        parsed = urlparse(u)
        if parsed.netloc != "" and parsed.netloc != netloc:
            extern_links.add(u)

    return list(extern_links)


def get_random_external_link(starting_page):
    bs = BeautifulSoup(urlopen(starting_page), "html.parser")
    external_links = get_external_links(bs, starting_page)
    if not len(external_links):
        print("No external links, look around the site for one")
        internal_links = get_internal_links(bs, starting_page)
        return get_random_external_link(random.choice(internal_links))
    else:
        return random.choice(external_links)


def follow_external_only(starting_page):
    external_link = get_random_external_link(starting_page)
    print(f"Random external link is: {external_link}")
    follow_external_only(external_link)


url = "https://www.oreilly.com/"
follow_external_only(url)

```

