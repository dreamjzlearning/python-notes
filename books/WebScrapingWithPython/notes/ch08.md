---
title: "Ch08 Scrapy"
category:
 - Programming
tag:
 - Python
 - Web Scraping
---

## 1. Installing 

```sh
# it is recommended to install in a virtual env
$ pip install Scrapy
```

### 1.1 Initializing a New Spider

```sh
# create a new spider in the current directory
$ scrapy startproject wikispider
```

```
CH08
└───wiki_spider
    │   scrapy.cfg
    │
    └───wiki_spider
        │   items.py
        │   middlewares.py
        │   pipelines.py
        │   settings.py
        │   __init__.py
        │
        └───spiders
                __init__.py
```

## 2. Writing a Simple Scraper

To create a crawler, you will add a new file inside the child `spiders` directory.

```python
# ch08\wiki_spider\wiki_spider\spiders\article.py

from scrapy import Spider, Request


class ArticleSpider(Spider):
    name = "article"

    def start_requests(self):
        urls = [
            "http://en.wikipedia.org/wiki/Python_%28programming_language%29",
            "https://en.wikipedia.org/wiki/Functional_programming",
            "https://en.wikipedia.org/wiki/Monty_Python",
        ]
        return [Request(url=url, callback=self.parse) for url in urls]

    def parse(self, response):
        url = response.url
        title = response.css("h1#firstHeading span::text").extract_first()
        print(f"URL is: {url}")
        print(f"Title is: {title}")

```

- `start_request`
  A Scrapy-defined entry point to the program used to generate Request objects that Scrapy uses to crawl the website.
- `parse`:
  A callback function defined by the user and passed to the Request object.
- `response.css`
  Uses a CSS selector to get elements. `::text` is used to get the text content of an element.

Running spider:

```sh
# in ch08\wiki_spider\
$ scrapy runspider wiki_spider\spiders\article.py
```

## 3. Spidering with Rules

```python
# ch08\wiki_spider\wiki_spider\spiders\article.py

from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class ArticleSpider(CrawlSpider):
    name = "articles"
    allowed_domains = ["wikipedia.org"]
    start_urls = ["https://en.wikipedia.org/wiki/Benevolent_dictator_for_life"]
    rules = [Rule(LinkExtractor(allow=r".*"), callback="parse_items", follow=True)]

    def parse_items(self, response):
        url = response.url
        title = response.css("span.mw-page-title-main::text").extract_first()
        text = response.xpath('//div[@id="mw-content-text"]//text()').extract()
        last_updated = response.css("li#footer-info-lastmod::text").extract_first()
        last_updated = last_updated.replace("'This page was last edited on ", "")

        print(f"URL is: {url}")
        print(f"Title is: {title}")
        print(f"Text is: {text}")
        print(f"Last updated: {last_updated}")

```

- `allowed_domains`: the domain of the link allowed
- `start_urls`: where to start crawling
- `Rule`: Rule object that defines the rule all founded links are filtered through
  - `link_extractor`: LinkExtractor object, is used to extract links with rules.
    - `allow`: Allow all links that match the provided regexp
    - `deny`: Deny all links that match the provided regexp
  -  `callback`: callback function be used to parse the content
  - `cd_kwargs`: `dict `arguments to be passed to the callback function
  - `follow`: Indicates whether the links found in the current page will be included in a future crawl
- `xpath`: XPath selector is used to retrieve text content **including text in child tags**.
  Using a CSS selector will ignore the child tags.

**Crawling only article pages**

```python
# ch08\wiki_spider\wiki_spider\spiders\article_more_rules.py

from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule


class ArticleSpider(CrawlSpider):
    name = "articles"
    allowed_domains = ["wikipedia.org"]
    start_urls = ["https://en.wikipedia.org/wiki/Benevolent_dictator_for_life"]
    rules = [
        Rule(  # only article links allowed
            LinkExtractor(allow=r"(/wiki/)((?!:).)*$"),
            callback="parse_items",
            follow=True,
            cb_kwargs={"is_article": True},
        ),
        Rule(  # all links
            LinkExtractor(allow=".*"),
            callback="parse_items",
            cb_kwargs={"is_article": False},
        ),
    ]

    def parse_items(self, response, is_article):
        title = response.css("span.mw-page-title-main::text").extract_first()
        if is_article:
            url = response.url
            text = response.xpath('//div[@id="mw-content-text"]//text()').extract()
            last_updated = response.css("li#footer-info-lastmod::text").extract_first()
            last_updated = last_updated.replace("'This page was last edited on ", "")
            print(f"URL is: {url}")
            print(f"Title is: {title}")
            print(f"Text is: {text}")
            print(f"Last updated: {last_updated}")
        else:
            print(f"{title} is not an article. URL: {response.url} ")

```

## 4. Creating Items

Define a new item `Article` in `items.py`:

```python
# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class WikiSpiderItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass


class Article(scrapy.Item):
    url = scrapy.Field()
    title = scrapy.Field()
    text = scrapy.Field()
    last_updated = scrapy.Field()

```

Create file `article_items.py`:

```python
# ch08\wiki_spider\wiki_spider\spiders\article_items.py

from scrapy.linkextractors import LinkExtractor
from scrapy.spiders import CrawlSpider, Rule
from wiki_spider.items import Article


class ArticleSpider(CrawlSpider):
    name = "article_items"
    allowed_domains = ["wikipedia.org"]
    start_urls = ["https://en.wikipedia.org/wiki/Benevolent_dictator_for_life"]
    rules = [
        Rule(  # only article links allowed
            LinkExtractor(allow=r"(/wiki/)((?!:).)*$"),
            callback="parse_items",
            follow=True,
        )
    ]

    def parse_items(self, response):
        article = Article()
        article["url"] = response.url
        article["title"] = response.css("span.mw-page-title-main::text").extract_first()
        article["text"] = response.xpath(
            '//div[@id="mw-content-text"]//text()'
        ).extract()
        last_updated = response.css("li#footer-info-lastmod::text").extract_first()
        article["last_updated"] = last_updated.replace(
            "'This page was last edited on ", ""
        )

        return article

```

Running the spider above, it will output the debugging data along with each article item as a dict.

## 5. Outputting Items

Items can be saved as in many ways such as CSV. JSON or XML.

```sh 
$ scrapy runspider articleItems.py -o articles.csv -t csv
$ scrapy runspider articleItems.py -o articles.json -t json
$ scrapy runspider articleItems.py -o articles.xml -t xml
```

- `-t`: specifies the format of the output file

## 6. The Item Pipeline

