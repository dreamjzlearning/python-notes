---
title: "Ch09 Storing Data"
category:
 - Programming
tag:
 - Python
 - Web Scraping
---

## 1. Media Files

```python
# 09.1.py
# Download python logo

from urllib.request import urlopen, urlretrieve
from bs4 import BeautifulSoup

bs = BeautifulSoup(urlopen("http://www.pythonscraping.com/"), "html.parser")
img_url = bs.find("img", {"alt": "python-logo"})["src"]
urlretrieve(img_url, "logo.jpg")

```

Download all files in the tagâ€™s `src`:

```python
# 09.2.py
# download all internal files

import os

from urllib.request import urlretrieve, urlopen
from urllib.parse import urlparse
from bs4 import BeautifulSoup

download_dir = "download"
base_url = "https://pythonscraping.com/"
base_netloc = urlparse(base_url).netloc


def get_abs_url(source):
    if urlparse(base_url).netloc == "":
        return base_url + source
    return source


def get_download_path(file_url):
    parsed = urlparse(file_url)
    netloc = parsed.netloc.strip("/")
    path = parsed.path.strip("/")
    local_file = f"{download_dir}/{netloc}/{path}"

    local_path = "/".join(local_file.split("/")[:-1])
    if not os.path.exists(local_path):
        os.makedirs(local_path)
    return local_file


bs = BeautifulSoup(urlopen(base_url), "html.parser")
download_list = bs.find_all(src=True)

for download in download_list:
    file_url = get_abs_url(download["src"])
    if file_url is not None:
        try:
            urlretrieve(file_url, get_download_path(file_url))
            print(file_url)
        except Exception as e:
            print(f"Could not retrieve {file_url} Error: {e}")
```

## 2. Storing Data to CSV

CSV (Comma-Separated Values) is one of the most popular formats in which to store spreadsheet data.

```
fruit,cost
apple,1.00
banana,0.30
pear,1.25
```

```python
# 09.3.py
# write csv file

import csv

csv_file = open("test.csv", "w+")
try:
    writer = csv.writer(csv_file)
    writer.writerow(("number", "number plus 2", "number times 2"))
    for i in range(10):
        writer.writerow((i, i + 2, i * 2))
finally:
    csv_file.close()

```

- `open`: if the file does not exist, it will be created. If it already exists, it will be overwritten.

One common web scraping task is to retrieve an HTML table and write it as a CSV file.

```python
# 09.4.py
# Save HTML table as CSV

import csv

from urllib.request import urlopen
from bs4 import BeautifulSoup

bs = BeautifulSoup(
    urlopen(
        "https://en.wikipedia.org/wiki/List_of_countries_with_McDonald%27s_restaurants"
    ),
    "html.parser",
)

table = bs.find("table", {"class": "wikitable"})
rows = table.find_all("tr")
csv_file = open("countries.csv", "w+", encoding="UTF-8")
writer = csv.writer(csv_file)
try:
    for row in rows:
        csv_row = []
        for cell in row.find_all(["td", "th"]):
            csv_row.append(cell.get_text().strip())
        writer.writerow(csv_row)
finally:
    csv_file.close()

```

## 3. MySQL

```python
# 09.5.py
# Connecting to MysQL

import pymysql

conn = pymysql.connect(host="xxx", port=4567, user="xxx", password="xxx", db="scraping")

cur = conn.cursor()
cur.execute("SELECT * FROM pages WHERE id=1")

print(cur.fetchone())

cur.close()
conn.close()

```

- `conn`: The connection object is responsible for connecting to the database, sending db information, handling rollbacks, and creating new cursor objects.
- `cur`: The cursor object keeps track of certain state information.
- Closing connections and cursors after finishing using them is essential to prevent connection leaks.

Create db and table with UTF-8:

```sql
-- create db
CREATE DATABASE IF NOT EXISTS scraping CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

-- select db
USE scraping;

-- create table pages
CREATE TABLE IF NOT EXISTS pages (
    id BIGINT(7) NOT NULL AUTO_INCREMENT,
    title VARCHAR(200),
    content VARCHAR(10000),
    created TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    PRIMARY KEY(id)
) CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;
```

Collecting data and storing it into MySQL:

```python
# 09.6.py
# Storing data into MySQL

import pymysql
import re
import random
import time

from urllib.request import urlopen
from bs4 import BeautifulSoup

random.seed(time.time())

conn = pymysql.connect(
    host="xxx",
    port=4567,
    user="xxx",
    password="xxx",
    charset="utf8",
)

cur = conn.cursor()
cur.execute("USE scraping")


def store(title, content):
    cur.execute(
        'INSERT INTO pages (title, content) VALUES ("%s", "%s")', (title, content)
    )
    cur.connection.commit()


def get_links(article_url):
    bs = BeautifulSoup(urlopen("http://en.wikipedia.org" + article_url), "html.parser")

    title = bs.find("h1").get_text()
    content = bs.find("div", {"id": "mw-content-text"}).find("p").get_text()
    store(title, content)

    return bs.find("div", {"id": "bodyContent"}).find_all(
        "a", href=re.compile(r"(/wiki/)((?!:).)*$")
    )


links = get_links("/wiki/Kevin_Bacon")
try:
    while len(links) > 0:
        new_article = links[random.randint(0, len(links) - 1)].attrs["href"]
        print(new_article)
        links = get_links(new_article)
finally:
    cur.close()
    conn.close()

```

### 3.1 Database Techniques and Good Practice

1. Always add autoincremented id columns that are used as the primary key to tables.
2. Using intelligent indexing.
3. Balance of query time and database size.

## 4. Email

```python
# 09.7.py
# Sending Email

import smtplib
from email.mime.text import MIMEText

msg = MIMEText("The body of the email")

msg["Subject"] = "An Email Alert"
msg["From"] = "xxx"
msg["To"] = "xxx"

try:
    with smtplib.SMTP("smtp.office365.com", 587) as server:
        server.starttls()
        server.login("xxx", "xxx")
        server.sendmail("xxx", "xxx", msg.as_string())
        print("Email sent successfully")
except Exception as e:
    print(f"Failed to send email: {e}")

```



