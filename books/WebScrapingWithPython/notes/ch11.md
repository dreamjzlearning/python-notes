---
title: "Ch11 Working with Dirty Data"
category:
 - Programming
tag:
 - Python
 - Web Scraping
---

## 1. Cleaning Text

```python
# 11.1.py
# Cleaning Text

from urllib.request import urlopen
from bs4 import BeautifulSoup

import re, string, unicodedata


def replace_newlines(text: str) -> str:
    return text.replace("\n", " ")


def make_lowercase(text: str) -> str:
    return text.lower()


def split_sentences(text: str) -> list[str]:
    return [s.strip() for s in text.split(". ")]


CITATION_RE = re.compile(r"\[0-9*\]")


def strip_citations(text: str) -> str:
    return re.sub(CITATION_RE, "", text)


PARENTS_RE = re.compile(r"\([a-z A-Z \+\.,\-]{0,100}\)")


def remove_parentheses(text: str) -> str:
    return re.sub(PARENTS_RE, "", text)


DESCRIPTION_RE = re.compile(r"\n[a-z A-Z]*:")


def remove_descriptions(text: str) -> str:
    return re.sub(DESCRIPTION_RE, "", text)


PUNCTUATION_RE = re.compile("|".join([re.escape(c) for c in string.punctuation]))


def remove_punctuation(text: str) -> str:
    return re.sub(PUNCTUATION_RE, "", text)


def normalize(text: str) -> str:
    return unicodedata.normalize("NFKD", text)


text_operations = [
    strip_citations,
    remove_parentheses,
    remove_descriptions,
    replace_newlines,
    make_lowercase,
    split_sentences,
    remove_punctuation,
    normalize,
]

url = "http://en.wikipedia.org/wiki/Python_(programming_language)"
bs = BeautifulSoup(urlopen(url), "html.parser")

content = bs.find("div", {"id": "mw-content-text"}).find_all("p")
content = [p.get_text() for p in content]

cleaned = content
for op in text_operations:
    for i in range(len(cleaned)):
        c = cleaned[i]
        if type(c) == list:
            c = [op(s) for s in c]
        else:
            c = op(c)
        cleaned[i] = c

print(cleaned)

```

## 2. Working with Normalized Text

One common technique to process clean text is to break it up into smaller pieces of text that can be more easily quantified and analyzed. Computational linguists call these *n-grams*, where n represents the number of words in each piece of text.

```python
# 11.2.py
# Breaking text into n-grams


def get_ngrams(text, n):
    text = text.split(" ")
    return [text[i : i + n] for i in range(len(text) - n + 1)]


ret = get_ngrams("web scraping with python", 2)
print(ret)

# output
# [['web', 'scraping'], ['scraping', 'with'], ['with', 'python']]
```

## 3. Clean Data with Pandas

`Pandas` is a Python lib for data analytics.

```sh
$ pip install pandas
```

```python
# 11.3.py
# DataFrame

import pandas as pd

df = pd.DataFrame([["a", 1], ["b", 2], ["c", 3]])
print(df.head())

# output
   0  1
0  a  1
1  b  2
2  c  3

```

### 3.1 Cleaning

```python
# 11.4.py

import pandas as pd
import re


df = pd.read_csv("countries.csv")

# head(n) displays first n rows
print(df.head(10))

# rename the column names
# inplace means that the cols are renamed in-place in the
# original DataFrame rather than a new DataFrame being returned.
df.rename(
    columns={
        "#": "Order",
        "Country/territory": "Country",
        "Date of first store": "Date",
        "First outlet location": "Location",
        "Max. no. ofoperatingoutlets": "Outlets",
    },
    inplace=True,
)

date_re = re.compile("[A-Z][a-z]+ [0-9]{1,2}, [0-9]{4}")
df["Date"] = df["Date"].apply(lambda d: date_re.findall(d)[0])
df["Date"] = pd.to_datetime(df["Date"])

print(df.head(10))

```

### 3.2 Indexing, Sorting and Filtering

We can set the index using the `set_index` method:

```python
df.set_index(["Order", inplce=True])
```

This discards the old index and moves the `Order` column into the index.

The `sort_values` method can be used to sort data by one or many columns.

```python
df.sort_values(by=["Outlets", "Date"], ascending=False)
```

Filtering DataFrames is easy with `query`:

```python
df.query("Outlets < 100")
df.query("Date is not None")
```



